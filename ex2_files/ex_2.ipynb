{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2232d0a0-c06e-430a-9eca-928881efed74",
   "metadata": {},
   "source": [
    "in this note book we implement excercise 2.\n",
    "\n",
    "\n",
    "we will be working interactivley:\n",
    "\n",
    "load variables into global and interact in jupyter. only things work and are understood, do we move to classes and fucntions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49fe2b-40f0-44f2-9230-4bae4a464030",
   "metadata": {},
   "source": [
    "# imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4597f9-6a8e-40ba-b0c8-e223cb5733f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ba8f2-ed8a-4417-8bee-7b25eb67a96f",
   "metadata": {},
   "source": "## brief summary of methods in helpers\n\n**Helper functions:**\n- `decision_tree_demo()`: Demo of training/predicting with DecisionTreeClassifier\n- `loading_random_forest()`: Shows how to initialize RandomForestClassifier with 300 trees, max_depth=6\n- `loading_xgboost()`: Shows how to initialize XGBClassifier with 300 trees, max_depth=6, lr=0.1\n- `plot_decision_boundaries(model, X, y, title)`: Visualizes classifier decision boundaries on 2D space\n- `knn_examples(X_train, Y_train, X_test, Y_test)`: Demo of training/predicting with KNNClassifier\n- `read_data_demo(filename)`: Reads CSV and returns numpy array + column names"
  },
  {
   "cell_type": "markdown",
   "id": "d1c60ff4-ed3c-4afc-9ce3-f0a32e40788b",
   "metadata": {},
   "source": [
    "## data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab3567-dadd-4ef5-8520-2d74b7aa42a0",
   "metadata": {},
   "source": "**Task:** Create paths to all CSV files (train.csv, validation.csv, test.csv, AD_test.csv). Print first few rows of train and AD_test. Print shapes of all datasets."
  },
  {
   "cell_type": "markdown",
   "id": "4345fbe5-18ae-4bb9-8351-34655e265789",
   "metadata": {},
   "source": [
    "# part 1: KNN iplementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a28e4a-246a-49f5-b1cf-2f011b5e20f3",
   "metadata": {},
   "source": [
    "specification notes\n",
    "\n",
    "Algorithm 1: kNN Classification\n",
    "\n",
    "Data: Training dataset Xtrain = {(xi, yi)}N\n",
    "\n",
    "i=1, where xi ∈ R2 is the\n",
    "\n",
    "feature vector and yi corresponds to its class label; Test instance\n",
    "ˆxtest; Number of neighbors k.\n",
    "\n",
    "Result: Predicted class label ˆytest for the test instance.\n",
    "for each training instance (xi, yi) do\n",
    "\n",
    "Calculate the distance di between xi and xtest.\n",
    "\n",
    "Sort the distances di in ascending order and select the first k instances.\n",
    "for each class c do\n",
    "\n",
    "Count the occurrences of c in the selected k instances.\n",
    "Assign the class label ˆytest to the one with the highest count.\n",
    "return ˆytest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e10a6-afde-4f8a-a3cb-69e1b183cb46",
   "metadata": {},
   "source": [
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k, distance_metric='l2'):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.X_train = None\n",
    "        self.Y_train = None\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        Update the kNN classifier with the provided training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train (numpy array) of size (N, d): Training feature vectors.\n",
    "        - Y_train (numpy array) of size (N,): Corresponding class labels.\n",
    "        \"\"\"\n",
    "        self.X_train = X_train.astype(np.float32)\n",
    "        self.Y_train = Y_train\n",
    "        d = self.X_train.shape[1]\n",
    "        if self.distance_metric == 'l2':\n",
    "            self.index = faiss.index_factory(d, \"Flat\", faiss.METRIC_L2)\n",
    "        elif self.distance_metric == 'l1':\n",
    "            self.index = faiss.index_factory(d, \"Flat\", faiss.METRIC_L1)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        pass\n",
    "        self.index.add(self.X_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the given data.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy array) of size (M, d): Feature vectors.\n",
    "\n",
    "        Returns:\n",
    "        - (numpy array) of size (M,): Predicted class labels.\n",
    "        \"\"\"\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "\n",
    "    def knn_distance(self, X):\n",
    "        \"\"\"\n",
    "        Calculate kNN distances for the given data. You must use the faiss library to compute the distances.\n",
    "        See lecture slides and https://github.com/facebookresearch/faiss/wiki/Getting-started#in-python-2 for more information.\n",
    "\n",
    "        Parameters:\n",
    "        - X (numpy array) of size (M, d): Feature vectors.\n",
    "\n",
    "        Returns:\n",
    "        - (numpy array) of size (M, k): kNN distances.\n",
    "        - (numpy array) of size (M, k): Indices of kNNs.\n",
    "        \"\"\"\n",
    "        X = X.astype(np.float32)\n",
    "\t#### YOUR CODE GOES HERE ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640363c4-673a-4d8c-880a-cd6091555360",
   "metadata": {},
   "source": [
    "things i need to understand:\n",
    "\n",
    "how does faiss knn work? what happens in fit?\n",
    "we need to be able to play with the internals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb71653-2042-4714-b09f-1bfce197e131",
   "metadata": {},
   "source": [
    "notes: since our data lives in 2 d, can we use heuristics to pull top k closest neightbours without going over all data?\n",
    "\n",
    "create some kind of lookup bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f68580-1aac-4473-8f54-4bde719f91a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8fd7644-728a-4c5f-8f4a-8db3337a5368",
   "metadata": {},
   "source": [
    "## KNN grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9865b59-ce50-4d69-b9e1-8809f42600dc",
   "metadata": {},
   "source": "**Grid search:** Train kNN for all combinations of k ∈ {1, 10, 100, 1000, 3000} × distance ∈ {L1, L2}. Create 5×2 table of test accuracies. Save all 10 models with their hyperparameters and test accuracies.\n\n**Questions:**\n1. What's the trend as k increases? Does it differ by distance metric?\n2. Visualize decision boundaries for: (i) L2 with k_max (best accuracy), (ii) L2 with k_min (worst accuracy), (iii) L1 with k_max\n   - (a) Compare k_max vs k_min with L2: why does k_max perform better?\n   - (b) Compare L2 vs L1 with k_max: how does distance metric affect the space?"
  },
  {
   "cell_type": "markdown",
   "id": "c37798c3-0eb1-47ae-ab2a-e313b5d841ae",
   "metadata": {},
   "source": [
    "think about the problem - what would you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7ed1b-53ae-4a0c-9659-b388a32f4bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d311c95-3f85-4863-9ffe-2e25696a7e65",
   "metadata": {},
   "source": [
    "### KNN decision boundaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3645b-59fd-4e09-aa12-9c3fc5b9f998",
   "metadata": {},
   "source": "**Task:** Use `plot_decision_boundaries()` helper to visualize 3 models on test data:\n1. L2 distance, k = k_max (highest test accuracy)\n2. L2 distance, k = k_min (lowest test accuracy)  \n3. L1 distance, k = k_max"
  },
  {
   "cell_type": "markdown",
   "id": "fe852eb2-7baa-4860-a888-b80c8a7a0d8a",
   "metadata": {},
   "source": "## Anomaly detection\n\n**Task:** Use AD_test.csv as test set, train.csv as single class (ignore labels).\n1. Find 5 nearest neighbors (L2 distance) for each AD_test sample\n2. Sum the 5 distances = anomaly score per sample\n3. Select top 50 samples with highest scores = anomalies, rest = normal\n4. Plot: AD_test colored by prediction (blue=normal, red=anomaly) + train.csv in black with alpha=0.01\n\n**Question:** What characterizes the anomalies? How do they differ from normal data?"
  },
  {
   "cell_type": "markdown",
   "id": "f2ff9845-2dee-4fea-b3f2-68cc354c2155",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "934e3feb-fe02-4377-bffc-07f595685cf0",
   "metadata": {},
   "source": [
    "# trees"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "8d86026c-ae06-4ef7-88f8-b9d48513b271",
   "metadata": {},
   "outputs": [],
   "source": "## Understanding trees - interactive exploration\n\n**Goal:** Interact with tree components to understand how predictions work.\n- Create a simple tree, examine its structure (tree_.feature, tree_.threshold, tree_.children_left/right, tree_.value)\n- Manually trace a prediction path for a sample point\n- Understand how tree.predict() navigates the tree structure\n\n**Visualization library:** `sklearn.tree.plot_tree()` or `graphviz` for tree visualization. For more advanced viz, use `dtreeviz` library."
  },
  {
   "cell_type": "code",
   "id": "3aijcnadqci",
   "source": "# Tree visualization series placeholder",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4yzzs4x68vr",
   "source": "## Visualizing tree complexity\n\n**Series 1 - Fixed depth=1, varying leaves:**\nTrain and visualize trees with max_depth=1 and max_leaf_nodes ∈ {2, 4, 8, 24, 50, 100}\n\n**Series 2 - Varying depth, controlled leaves:**\nTrain and visualize trees with max_depth ∈ {2, 4, 8, 16, 32, 50} and appropriate max_leaf_nodes to observe depth effects\n\n**Goal:** Understand how depth vs leaf constraints affect tree structure and decision boundaries.\n\n**Visualization:** Use `sklearn.tree.plot_tree()` for tree structure or `plot_decision_boundaries()` for geographic space.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "85450dee-9e03-42b9-a915-aca5a5f1e154",
   "metadata": {},
   "source": "## train 24 trees\n\n**Task:** Train DecisionTreeClassifier for all combinations of max_depth ∈ {1, 2, 4, 6, 10, 20, 50, 100} × max_leaf_nodes ∈ {50, 100, 1000}. Save all 24 models with their hyperparameters (max_depth, max_leaf_nodes) and accuracies (train, validation, test)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae74f7-7122-4530-9c2a-8f4eb94fc738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fc7b504-2620-49f3-9c4c-6ba111c8e029",
   "metadata": {},
   "source": [
    "## tree analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100e318-b5dd-408d-bfb2-98674fdae772",
   "metadata": {},
   "source": "**Questions:**\n1. Report test accuracy of tree with best validation accuracy\n2. Does this tree generalize well? Is validation set sufficient for selection?\n3. Are 50 leaf nodes enough for perfect accuracy with 50 states? Why/why not?\n4. Visualize decision boundaries for best tree from Q1. What shapes do classes form?\n5. Find best tree with exactly 50 leaf nodes. Visualize and compare to Q1 best tree.\n6. Find best tree with max_depth ≤ 6. Visualize and compare space division to Q4."
  },
  {
   "cell_type": "markdown",
   "id": "11f6604b-51a4-4073-98fe-a7fb917c55f2",
   "metadata": {},
   "source": [
    "think and research - how many leaves do trees? how many boundaries can a tree make? how many hyper planes?\n",
    "\n",
    "ask chat gpt for deep think"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9746815-6cf6-4507-adc6-c9a7509780d7",
   "metadata": {},
   "source": [
    "## random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03b78f-6f2c-43b2-afac-29a153b44088",
   "metadata": {},
   "source": "**Task:** Train RandomForestClassifier with n_estimators=300, max_depth=6. Visualize decision boundaries. \n\n**Question:** Is this model more expressive than single tree from Q1? How does visualization show this?"
  },
  {
   "cell_type": "markdown",
   "id": "59ea7c1f-4639-4ffe-873b-87842cc4e0cd",
   "metadata": {},
   "source": [
    "# bonus: boosted trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecaa860-800d-4189-8b59-b68a43f0dab9",
   "metadata": {},
   "source": "**Bonus (5 pts):** Train XGBClassifier with n_estimators=300, max_depth=6, learning_rate=0.1. Report test accuracy and visualize predictions. Compare to random forest: how do predictions differ? Which performs better?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da80513-2a81-462f-afab-e47b305be6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff5c4634-9135-4d25-ac39-28c81d23eac3",
   "metadata": {},
   "source": [
    "## interpret Ml explainable boosting classifier\n",
    "\n",
    "instead of xgb trees, use the glassbox EBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a988d2-cc9f-4ad3-931e-932d53df3b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}