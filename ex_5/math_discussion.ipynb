{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49e3f327-3543-431b-899e-e15a18ab16a1",
   "metadata": {},
   "source": [
    "is the line-by-line breakdown of how the code maps to the math:\n",
    "\n",
    "  1. The Log-Likelihood Equation (PDF Page 4)\n",
    "  The PDF breaks down the log-probability of a single Gaussian component $\\log p(\\mathbf{x}|k)$ into three parts:\n",
    "  $$ \\log p(\\mathbf{x}|k) = \\underbrace{-\\log(2\\pi)}{\\text{Constant}} \\underbrace{-\\log(\\sigma{k1}) - \\log(\\sigma_{k2})}_{\\text{Determinant Term}} \\underbrace{- \\frac{1}{2} \\left[ \\frac{(x_1 -\n",
    "  \\mu_{k1})^2}{\\sigma_{k1}^2} + \\frac{(x_2 - \\mu_{k2})^2}{\\sigma_{k2}^2} \\right]}_{\\text{Mahalanobis / Exponential Term}} $$\n",
    "\n",
    "  2. Mapping Code to Math\n",
    "\n",
    "  A. The \"Mahalanobis\" Term (The Bracketed Part)\n",
    "   * Math: $\\frac{(x_1 - \\mu_{k1})^2}{\\sigma_{k1}^2} + \\frac{(x_2 - \\mu_{k2})^2}{\\sigma_{k2}^2}$\n",
    "   * Code:\n",
    "\n",
    "   1     # diff represents (x - μ)\n",
    "   2     # variances represents σ²\n",
    "   3     mahalanobis = ((diff ** 2) / variances.unsqueeze(0)).sum(dim=2)\n",
    "       * diff ** 2 is the numerator $(x - \\mu)^2$.\n",
    "       * / variances divides by the denominator $\\sigma^2$.\n",
    "       * .sum(dim=2) sums the two dimensions ($x_1$ and $x_2$), completing the expression inside the brackets.\n",
    "\n",
    "  B. The Determinant Term (Log Sigmas)\n",
    "   * Math: $-\\log(\\sigma_{k1}) - \\log(\\sigma_{k2})$\n",
    "   * Code:\n",
    "\n",
    "   1     # self.log_variances stores log(σ²) which equals 2*log(σ)\n",
    "   2     log_det = 0.5 * self.log_variances.sum(dim=1)\n",
    "       * Summing self.log_variances gives $\\log(\\sigma_{k1}^2) + \\log(\\sigma_{k2}^2) = 2\\log(\\sigma_{k1}) + 2\\log(\\sigma_{k2})$.\n",
    "       * Multiplying by 0.5 gives $\\log(\\sigma_{k1}) + \\log(\\sigma_{k2})$.\n",
    "       * Subtracting it later (line 79) matches the negative signs in the equation.\n",
    "\n",
    "  C. The Constant Term\n",
    "   * Math: $-\\log(2\\pi)$\n",
    "   * Code:\n",
    "\n",
    "   1     -math.log(2 * math.pi)\n",
    "       * Note: Ensure you `import math` at the top of your file for this to work.\n",
    "\n",
    "  D. The Final Mixture (LogSumExp)\n",
    "   * Math: $\\log p(\\mathbf{x}) = \\log \\sum_{k=1}^K \\exp [\\log p(k) + \\log p(\\mathbf{x}|k)]$\n",
    "   * Code:\n",
    "\n",
    "   1     log_joint = log_pi + log_p_x_given_k  # Inner term: log p(k) + log p(x|k)\n",
    "   2     log_likelihood = torch.logsumexp(log_joint, dim=1) # Outer term: log sum exp\n",
    "       * This correctly implements the numerically stable \"Log-Sum-Exp\" trick required by the PDF to avoid underflow when probabilities are very small.\n",
    "\n",
    "  Conclusion\n",
    "  The code is a faithful translation of the PDF's derivation for a 2D GMM with a diagonal covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4c31e5-201c-40ad-94f8-df1da85cfd48",
   "metadata": {},
   "source": [
    "\n",
    "  Let's separate them to make it crystal clear.\n",
    "\n",
    "  1. The Probability Definition\n",
    "  Your intuition is correct.\n",
    "  The probability of a single sample $x$ is indeed a weighted sum.\n",
    "  $$p(x) = \\sum_{k=1}^{K} \\underbrace{p(k)}_{\\text{weight } \\pi_k} \\cdot \\underbrace{p(x|k)}_{\\text{gaussian density}}$$\n",
    "\n",
    "   * Theorem Name: This is called the Law of Total Probability.\n",
    "   * Notation: $p(x|k)$ is the probability density of $x$ assuming it came from Gaussian $k$. Whether you write it as $p(x|k)$ or $\\mathcal{N}(x | \\mu_k, \\Sigma_k)$, it means the same thing.\n",
    "\n",
    "  ---\n",
    "\n",
    "  2. The Log-Sum-Exp Trick (The \"How\")\n",
    "\n",
    "  You described the process of turning multiplication into addition ($log(a \\cdot b) = log(a) + log(b)$). That is standard log-properties.\n",
    "\n",
    "  The Log-Sum-Exp trick is specifically about step 2: The Sum.\n",
    "\n",
    "  Here is the problem we are solving:\n",
    "  We want to calculate the log-likelihood: $\\log p(x)$.\n",
    "  $$ \\log p(x) = \\log \\left( \\sum_{k=1}^{K} p(k) \\cdot p(x|k) \\right) $$\n",
    "\n",
    "  If $p(k) \\cdot p(x|k)$ is tiny (like $10^{-100}$), the computer rounds it to 0 (underflow). If we sum a bunch of zeros, we get 0. $\\log(0)$ is $-\\infty$. The code crashes.\n",
    "\n",
    "  Here is the actual 3-step breakdown of Log-Sum-Exp:\n",
    "\n",
    "  Step 1: Work in Log-Space (The Multiplication fix)\n",
    "  We never calculate $p(k)$ or $p(x|k)$ directly. We calculate their logs.\n",
    "  Let $y_k$ be the log-probability of component $k$:\n",
    "  $$ y_k = \\log(p(k)) + \\log(p(x|k)) $$\n",
    "  This is just standard math: turning multiplication into addition.\n",
    "\n",
    "  Step 2: The Summation Problem\n",
    "  Now we need to sum them to get $p(x)$, but our values are currently logs. To sum them, we have to exponentiate them back:\n",
    "  $$ \\log p(x) = \\log \\left( \\sum_{k=1}^{K} e^{y_k} \\right) $$\n",
    "  Problem: If $y_k = -1000$, then $e^{-1000}$ underflows to 0. We are back to the start.\n",
    "\n",
    "  Step 3: The \"Shift\" Trick (The Actual Log-Sum-Exp)\n",
    "  We pull out the maximum value ($y_{max}$) from the sum. This is the \"magic\" algebraic step:\n",
    "  $$ \\log \\left( \\sum e^{y_k} \\right) = \\log \\left( \\sum e^{y_k - y_{max} + y_{max}} \\right) = \\log \\left( e^{y_{max}} \\sum e^{y_k - y_{max}} \\right) $$\n",
    "  $$ = y_{max} + \\log \\left( \\sum_{k=1}^{K} e^{y_k - y_{max}} \\right) $$\n",
    "\n",
    "  Why does this work?\n",
    "   * One of the terms will be $e^{y_{max} - y_{max}} = e^0 = 1$.\n",
    "   * Since we have at least one $1$ in the sum, the sum is never $0$.\n",
    "   * $\\log(\\text{something} \\ge 1)$ is always safe (never $-\\infty$).\n",
    "\n",
    "  Summary\n",
    "   1. Log: Turn multiplication into addition ($ \\log \\pi + \\log \\mathcal{N} $).\n",
    "   2. Sum: We need to sum the probabilities (Law of Total Probability).\n",
    "   3. Exp (Trick): We subtract the max value before exponentiating, sum them, log them, and add the max value back. This guarantees numerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f40552-48ac-4ebf-810e-91dc31965ec6",
   "metadata": {},
   "source": [
    "✦ Based on the PDF and the code skeleton, here are the specifics for the Gaussian dimensions and covariance:\n",
    "\n",
    "  1. Dimensionality\n",
    "  Each Gaussian is 2D. \n",
    "   * The data $X$ has shape (n_samples, 2).\n",
    "   * The means self.means have shape (n_components, 2).\n",
    "\n",
    "  2. Covariance Matrix\n",
    "  You do not need to implement a full $2 \\times 2$ covariance matrix. The PDF explicitly simplifies this for you.\n",
    "\n",
    "  Page 2 states: \n",
    "  > \"In this exercise, we will limit ourselves to a 2D diagonal covariance matrix.\"\n",
    "\n",
    "  A diagonal covariance matrix means we assume the dimensions (x and y) are independent. Instead of a matrix, you only need to store the variance for each dimension.\n",
    "\n",
    "  3. Shape and Parameters\n",
    "  In your code, the covariance is represented by self.log_variances:\n",
    "   * Shape: (n_components, 2)\n",
    "   * Why 2? Because for each of your $K$ Gaussians, you have one variance for the $x$ dimension ($\\sigma_{k1}^2$) and one variance for the $y$ dimension ($\\sigma_{k2}^2$).\n",
    "\n",
    "  If this were a \"full\" covariance model, the shape would be (n_components, 2, 2). But because it is diagonal, the off-diagonal elements are zero, so we only store the 2 diagonal elements per\n",
    "  component.\n",
    "\n",
    "  Summary of Shapes:\n",
    "   * `self.means`: (K, 2) -> Each Gaussian has a center $(x, y)$.\n",
    "   * `self.log_variances`: (K, 2) -> Each Gaussian has a \"spread\" in $x$ and a \"spread\" in $y$.\n",
    "   * `self.weights`: (K,) -> Each Gaussian has one scalar weight (logit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbf96c-633d-4c5d-a5e3-1a6b9f101641",
   "metadata": {},
   "source": [
    " 1. The Determinant of a Diagonal Matrix\n",
    "  First, let's look at your Covariance Matrix, $\\Sigma$. Since it is diagonal (as specified in the PDF), it looks like this:\n",
    "\n",
    "  $$ \\Sigma = \\begin{bmatrix} \\sigma_{1}^2 & 0 \\\\ 0 & \\sigma_{2}^2 \\end{bmatrix} $$\n",
    "\n",
    "  The determinant ($|\\Sigma|$) of a diagonal matrix is simply the product of its diagonal elements:\n",
    "  $$ |\\Sigma| = \\sigma_{1}^2 \\cdot \\sigma_{2}^2 $$\n",
    "\n",
    "  2. The Square Root Term\n",
    "  The formula has $|\\Sigma|^{1/2}$ (the square root of the determinant).\n",
    "  $$ |\\Sigma|^{1/2} = \\sqrt{\\sigma_{1}^2 \\cdot \\sigma_{2}^2} = \\sigma_{1} \\cdot \\sigma_{2} $$\n",
    "\n",
    "  So, the normalization term $\\frac{1}{|\\Sigma|^{1/2}}$ is actually just $\\frac{1}{\\sigma_1 \\sigma_2}$. This makes intuitive sense: the \"volume\" of the\n",
    "  Gaussian is scaled by the spread in dimension 1 times the spread in dimension 2.\n",
    "\n",
    "  3. Moving to Log-Space\n",
    "  Now, we take the negative log (because we are calculating log-likelihood):\n",
    "\n",
    "  $$ -\\log(|\\Sigma|^{1/2}) $$\n",
    "  $$ = -\\log(\\sigma_1 \\cdot \\sigma_2) $$\n",
    "\n",
    "  Using the log rule $\\log(a \\cdot b) = \\log(a) + \\log(b)$:\n",
    "  $$ = -(\\log(\\sigma_1) + \\log(\\sigma_2)) $$\n",
    "  $$ = -\\log(\\sigma_1) - \\log(\\sigma_2) $$\n",
    "\n",
    "  4. Connection to Your Code\n",
    "  In your code, you have self.log_variances.\n",
    "   * self.log_variances[0] holds $\\log(\\sigma_1^2)$.\n",
    "   * self.log_variances[1] holds $\\log(\\sigma_2^2)$.\n",
    "\n",
    "  The code does:\n",
    "\n",
    "   1 0.5 * self.log_variances.sum(dim=1)\n",
    "  Let's expand that math:\n",
    "  $$ 0.5 \\cdot (\\log(\\sigma_1^2) + \\log(\\sigma_2^2)) $$\n",
    "  Using the log rule $\\log(x^2) = 2\\log(x)$:\n",
    "  $$ 0.5 \\cdot (2\\log(\\sigma_1) + 2\\log(\\sigma_2)) $$\n",
    "  $$ = \\log(\\sigma_1) + \\log(\\sigma_2) $$\n",
    "\n",
    "  So the code calculates exactly the term derived above!\n",
    "\n",
    "  Answering your specific questions:\n",
    "\n",
    "  > \"How does log(|sigma_k|0.5) become the two separate values?\"**\n",
    "\n",
    "  Because the determinant of a diagonal matrix is a product, and the log of a product is a sum.\n",
    "\n",
    "  > \"Shouldn't the scaling by var be per feature?\"\n",
    "\n",
    "  Yes, and it is!\n",
    "   * In the exponential part (Mahalanobis distance), you divide by variance per feature: $\\frac{(x-\\mu)^2}{\\sigma^2}$.\n",
    "   * In the determinant part (the one we just discussed), you are essentially calculating the \"volume\" of the Gaussian. The volume is determined by\n",
    "     multiplying the spread of feature 1 by the spread of feature 2 ($\\sigma_1 \\cdot \\sigma_2$). When you take the log, that multiplication becomes addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538feef-dd98-44ea-aa28-6c4d5fe03577",
   "metadata": {},
   "source": [
    "\n",
    "✦ This derivation works out neatly because the covariance matrix $\\Sigma$ is diagonal.\n",
    "\n",
    "  Here is the step-by-step linear algebra that transforms the matrix notation into the simple sum of squared errors.\n",
    "\n",
    "  1. The Setup\n",
    "  Let's define our vectors and matrices:\n",
    "   * Difference Vector: $ \\mathbf{d} = (\\mathbf{x} - \\mathbf{\\mu}) = \\begin{bmatrix} x_1 - \\mu_1 \\\\ x_2 - \\mu_2 \\end{bmatrix} $\n",
    "   * Covariance Matrix (Diagonal): $ \\Sigma = \\begin{bmatrix} \\sigma_1^2 & 0 \\\\ 0 & \\sigma_2^2 \\end{bmatrix} $\n",
    "\n",
    "  2. The Inverse Covariance Matrix ($\\Sigma^{-1}$)\n",
    "  The formula uses the inverse of the covariance matrix.\n",
    "  For a diagonal matrix, the inverse is simply the reciprocal of the diagonal elements (and the zeros stay zeros).\n",
    "\n",
    "  $$ \\Sigma^{-1} = \\begin{bmatrix} \\frac{1}{\\sigma_1^2} & 0 \\\\ 0 & \\frac{1}{\\sigma_2^2} \\end{bmatrix} $$\n",
    "\n",
    "  3. The Matrix Multiplication\n",
    "  Now we compute the inner part: $(\\mathbf{x} - \\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu})$.\n",
    "\n",
    "  Step A: Compute $\\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu})$\n",
    "  Multiply the inverse matrix by the difference vector:\n",
    "  $$ \\begin{bmatrix} \\frac{1}{\\sigma_1^2} & 0 \\\\ 0 & \\frac{1}{\\sigma_2^2} \\end{bmatrix} \\begin{bmatrix} x_1 - \\mu_1 \\\\ x_2 - \\mu_2 \\end{bmatrix} =\n",
    "  \\begin{bmatrix} \\frac{x_1 - \\mu_1}{\\sigma_1^2} \\\\ \\frac{x_2 - \\mu_2}{\\sigma_2^2} \\end{bmatrix} $$\n",
    "  Notice how the zeros in the matrix prevented $x_1$ and $x_2$ from mixing.\n",
    "\n",
    "  Step B: Compute the Dot Product\n",
    "  Now multiply the row vector $(\\mathbf{x} - \\mathbf{\\mu})^T$ by the result from Step A:\n",
    "  $$ \\begin{bmatrix} (x_1 - \\mu_1) & (x_2 - \\mu_2) \\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{x_1 - \\mu_1}{\\sigma_1^2} \\\\ \\frac{x_2 - \\mu_2}{\\sigma_2^2}\n",
    "  \\end{bmatrix} $$\n",
    "\n",
    "  $$ = (x_1 - \\mu_1) \\cdot \\frac{(x_1 - \\mu_1)}{\\sigma_1^2} + (x_2 - \\mu_2) \\cdot \\frac{(x_2 - \\mu_2)}{\\sigma_2^2} $$\n",
    "\n",
    "  $$ = \\frac{(x_1 - \\mu_1)^2}{\\sigma_1^2} + \\frac{(x_2 - \\mu_2)^2}{\\sigma_2^2} $$\n",
    "\n",
    "  4. The Final Result\n",
    "  Multiply by the $-0.5$ from the outside:\n",
    "  $$ -\\frac{1}{2} \\left( \\frac{(x_1 - \\mu_1)^2}{\\sigma_1^2} + \\frac{(x_2 - \\mu_2)^2}{\\sigma_2^2} \\right) $$\n",
    "\n",
    "  Key Takeaway:\n",
    "  If the covariance matrix were not diagonal (i.e., if the off-diagonal zeros were numbers like $0.5$), you would have ended up with \"cross terms\" like $(x_1\n",
    "  - \\mu_1)(x_2 - \\mu_2)$. Because we assumed a diagonal covariance, those terms disappear, and the formula simplifies to just the sum of the normalized\n",
    "  squared distances for each dimension independently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da6b10c-b2e2-4b9a-b254-052035f5f485",
   "metadata": {},
   "source": [
    "# UMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c694cb-0931-4557-86ac-cf5da8c79347",
   "metadata": {},
   "source": [
    "✦ Just as we decomposed the GMM, let's break down the Uniform Mixture Model (UMM) using the exact same logic.\n",
    "\n",
    "  The high-level structure remains identical:\n",
    "   1. Log: Work in log-space.\n",
    "   2. Sum: Law of Total Probability (weighted sum of components).\n",
    "   3. Exp: Use Log-Sum-Exp for stability.\n",
    "\n",
    "  The only thing that changes is Step 1: How we calculate $\\log p(\\mathbf{x}|k)$ (the probability of a point given a specific component).\n",
    "\n",
    "  ---\n",
    "\n",
    "  1. The Density Derivation: From Area to Log-Prob\n",
    "\n",
    "  In a GMM, the probability depends on distance from the mean.\n",
    "  In a UMM, the probability is constant if you are inside the box, and zero if you are outside.\n",
    "\n",
    "  A. The \"Inside\" Case\n",
    "   * Geometry: A 2D uniform distribution is a rectangle (box).\n",
    "   * Parameters: Center $\\mathbf{c} = (c_1, c_2)$, Size $\\mathbf{s} = (s_1, s_2)$.\n",
    "   * Area: $Area = s_1 \\times s_2$.\n",
    "   * Probability Density: To be a valid distribution, the volume under the square must sum to 1. Since the height is constant:\n",
    "      $$ p(\\mathbf{x}|k) = \\frac{1}{\\text{Area}} = \\frac{1}{s_1 \\cdot s_2} $$\n",
    "   * Log-Probability:\n",
    "      $$ \\log p(\\mathbf{x}|k) = \\log(1) - \\log(s_1 \\cdot s_2) $$\n",
    "      $$ = 0 - (\\log s_1 + \\log s_2) $$\n",
    "      $$ = -\\log s_1 - \\log s_2 $$\n",
    "\n",
    "  Connection to Code:\n",
    "  Your model has self.log_sizes (which stores $\\log s$).\n",
    "  So, the log-probability is simply:\n",
    "\n",
    "   1 # Summing the log_sizes for both dimensions\n",
    "   2 log_prob_inside = -self.log_sizes.sum(dim=1) \n",
    "  Note: This is mathematically identical to the GMM determinant term, but without the $2\\pi$ or the Mahalanobis distance.\n",
    "\n",
    "  ---\n",
    "\n",
    "  2. The \"Hard\" Switch (The Mask)\n",
    "\n",
    "  This is the biggest difference. GMMs trail off to infinity; UMMs hit a wall.\n",
    "\n",
    "  The Condition:\n",
    "  A point $x$ is inside the box if it is within the range $[c - \\frac{s}{2}, c + \\frac{s}{2}]$ for both dimensions.\n",
    "\n",
    "  Mathematically, it's easier to check the distance from the center:\n",
    "  $$ |x_{dim} - c_{dim}| \\le \\frac{s_{dim}}{2} $$\n",
    "\n",
    "  The Log-Likelihood Logic:\n",
    "  $$ \\log p(\\mathbf{x}|k) = \\begin{cases} -\\log s_1 - \\log s_2 & \\text{if inside} \\\\ -\\infty & \\text{if outside} \\end{cases} $$\n",
    "\n",
    "  In computer code, we replace $-\\infty$ with a large negative number (like -1e6) to avoid NaNs during optimization.\n",
    "\n",
    "  ---\n",
    "\n",
    "  3. The Forward Implementation Plan\n",
    "\n",
    "  Here is how the concepts map to the tensor operations you will need:\n",
    "\n",
    "   1. Expand Dimensions: Just like GMM, you need (N, 1, 2) for data and (1, K, 2) for centers/sizes to compute all pairs.\n",
    "   2. Check Boundaries (The Mask):\n",
    "       * Calculate distance: dist = |x - center|\n",
    "       * Calculate limit: limit = size / 2 (remember size = exp(log_size))\n",
    "       * Check if dist <= limit for both dimensions.\n",
    "   3. Apply Logic:\n",
    "       * Start with the \"Inside\" probability (calculated in step 1).\n",
    "       * Where the mask says \"outside\", overwrite the value with -1e6.\n",
    "   4. Mixture (Same as GMM): Add log_pi, then logsumexp.\n",
    "\n",
    "  Summary Comparison\n",
    "\n",
    "\n",
    "  ┌───────────────────┬─────────────────────────────────────────────────┬───────────────────────────────────────────────┐\n",
    "  │ Concept           │ GMM                                             │ UMM                                           │\n",
    "  ├───────────────────┼─────────────────────────────────────────────────┼───────────────────────────────────────────────┤\n",
    "  │ Probability Shape │ Bell Curve (Smooth)                             │ Box (Hard edges)                              │\n",
    "  │ \"Distance\" Term   │ Mahalanobis (Quadratic decay)                   │ Mask (Binary check: Inside/Outside)           │\n",
    "  │ Normalization     │ $-\\log(\\sigma_1) - \\log(\\sigma_2) - \\text{const}$ │ $-\\log(s_1) - \\log(s_2)$                        │\n",
    "  │ Outside Bounds    │ Probability gets tiny, but $>0$                 │ Probability is exactly $0$ ($\\log = -\\infty$) │\n",
    "  └───────────────────┴─────────────────────────────────────────────────┴───────────────────────────────────────────────┘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
