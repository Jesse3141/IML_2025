{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Machine Learning Methods\n",
    "\n",
    "In this exercise, we will experiment with Multi-Layer Perceptron (MLP) and Convolutional Neural Network (CNN) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add current directory to path to import helpers if needed\n",
    "sys.path.append(os.getcwd())\n",
    "try:\n",
    "    from helpers import *\n",
    "except ImportError:\n",
    "    print(\"helpers.py not found or error importing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data\n",
    "## 5.1 European Countries\n",
    "The dataset consists of train.csv, validation.csv, test.csv. \n",
    "Columns: longitude (long), latitude (lat), country (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountryDataset(Dataset):\n",
    "    def __init__(self, split='train'):\n",
    "        filename = f\"{split}.csv\"\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"Warning: {filename} not found. Make sure you are in the correct directory.\")\n",
    "            self.X = torch.empty(0, 2)\n",
    "            self.y = torch.empty(0, dtype=torch.long)\n",
    "            return\n",
    "            \n",
    "        self.data = pd.read_csv(filename)\n",
    "        # Features: long, lat\n",
    "        self.X = torch.tensor(self.data[['long', 'lat']].values, dtype=torch.float32)\n",
    "        # Labels: country\n",
    "        self.y = torch.tensor(self.data['country'].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Multi-Layer Perceptrons\n",
    "## 6.1 Optimization of an MLP\n",
    "### 6.1.1 Task\n",
    "Implement a training pipeline from scratch. \n",
    "Model: 6 layers (includes input but not output -> 7 nn.Linear instances). Activation: ReLU. Batch Norm before activation.\n",
    "Default parameters provided in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=2, output_dim=5, hidden_dim=16, num_layers=6, use_batch_norm=False):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # According to instructions: \"6 layers, it includes the input layer but not the output layer (therefore, you'll initialize 7 nn.Linear instances in total)\"\n",
    "        # So we have:\n",
    "        # Linear(input, hidden)\n",
    "        # Linear(hidden, hidden) x (num_layers - 1)\n",
    "        # Linear(hidden, output)\n",
    "        # Total linear layers = 1 + (num_layers - 1) + 1 = num_layers + 1. \n",
    "        # If num_layers=6, then 7 linear layers.\n",
    "        \n",
    "        dims = [input_dim] + [hidden_dim] * num_layers + [output_dim]\n",
    "        \n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            # Add activation and BN for all but the last layer\n",
    "            if i < len(dims) - 2:\n",
    "                if use_batch_norm:\n",
    "                    layers.append(nn.BatchNorm1d(dims[i+1]))\n",
    "                layers.append(nn.ReLU())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPTrainer:\n",
    "    def __init__(self, \n",
    "                 learning_rate=1e-2, \n",
    "                 num_epochs=50, \n",
    "                 batch_size=32, \n",
    "                 use_batch_norm=False,\n",
    "                 hidden_dim=16,\n",
    "                 num_layers=6,\n",
    "                 seed=42):\n",
    "        \n",
    "        set_seed(seed)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Data\n",
    "        self.train_ds = CountryDataset('train')\n",
    "        self.val_ds = CountryDataset('validation')\n",
    "        self.test_ds = CountryDataset('test')\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.train_loader = DataLoader(self.train_ds, batch_size=batch_size, shuffle=True)\n",
    "        self.val_loader = DataLoader(self.val_ds, batch_size=batch_size, shuffle=False)\n",
    "        self.test_loader = DataLoader(self.test_ds, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Model\n",
    "        # Infer output dim from data labels (assuming 0 to N-1 labels)\n",
    "        output_dim = len(torch.unique(self.train_ds.y))\n",
    "        self.model = MLP(input_dim=2, output_dim=output_dim, \n",
    "                         hidden_dim=hidden_dim, num_layers=num_layers, \n",
    "                         use_batch_norm=use_batch_norm).to(self.device)\n",
    "        \n",
    "        # Optimization\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.num_epochs = num_epochs\n",
    "        self.history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for X, y in self.train_loader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(X)\n",
    "                loss = self.criterion(outputs, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item() * X.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "            \n",
    "            avg_train_loss = train_loss / total\n",
    "            train_acc = correct / total\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.evaluate(self.val_loader)\n",
    "            \n",
    "            self.history['train_loss'].append(avg_train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            \n",
    "            # print(f\"Epoch {epoch+1}/{self.num_epochs}: Train Loss {avg_train_loss:.4f}, Val Loss {val_loss:.4f}\")\n",
    "            \n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in loader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                outputs = self.model(X)\n",
    "                loss = self.criterion(outputs, y)\n",
    "                total_loss += loss.item() * X.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "        return total_loss / total, correct / total\n",
    "\n",
    "    def plot_results(self):\n",
    "        epochs = range(1, self.num_epochs + 1)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.history['train_loss'], label='Train')\n",
    "        plt.plot(epochs, self.history['val_loss'], label='Val')\n",
    "        plt.title('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.history['train_acc'], label='Train')\n",
    "        plt.plot(epochs, self.history['val_acc'], label='Val')\n",
    "        plt.title('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Questions\n",
    "1. **Learning Rate**: Train with 1, 0.01, 0.001, 0.00001. Plot validation loss.\n",
    "2. **Epochs**: Train for 100 epochs. Plot loss.\n",
    "3. **Batch Norm**: Add batch norm. Compare.\n",
    "4. **Batch Size**: 1, 16, 128, 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 Learning Rate\n",
    "lrs = [1.0, 0.01, 0.001, 0.00001]\n",
    "histories = {}\n",
    "for lr in lrs:\n",
    "    print(f\"Training with LR={lr}\")\n",
    "    trainer = MLPTrainer(learning_rate=lr, num_epochs=50)\n",
    "    trainer.train()\n",
    "    histories[lr] = trainer.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "for lr, loss in histories.items():\n",
    "    plt.plot(loss, label=f'LR={lr}')\n",
    "plt.legend()\n",
    "plt.title('Validation Loss per LR')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Evaluating MLPs Performance\n",
    "Train 6 classifiers for combinations of depth and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinations from Table 2\n",
    "configs = [
    "    {'depth': 1, 'width': 16},
    "    {'depth': 2, 'width': 16},
    "    {'depth': 6, 'width': 16},
    "    {'depth': 10, 'width': 16},
    "    {'depth': 6, 'width': 8},
    "    {'depth': 6, 'width': 32},
    "    {'depth': 6, 'width': 64}
    "]
    "\n",
    "# Implement loop to train these models and store results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Convolutional Neural Networks\n",
    "## 7.4 Task\n",
    "1. XGBoost\n",
    "2. Training from Scratch (ResNet18)\n",
    "3. Linear Probing\n",
    "4. Sklearn Probing\n",
    "5. Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for CNN tasks\n",
    "# You might need to import from cnn.py if you implement models there\n",
    "# from cnn import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
